```text
状态价值函数和动作价值函数：通过简单推导就可以分别得到两个价值函数的贝尔曼期望方程（Bellman Expectation Equation）
```

在强化学习中，**状态价值函数**和**动作价值函数**都可以从其定义出发，通过对“下一步回报”的条件期望进行展开，得到各自的**贝尔曼期望方程（Bellman Expectation Equation）**。下面给出清晰、标准的推导思路。

---

## 一、基本设定与符号

假设环境为一个马尔可夫决策过程（MDP）：

* 状态空间：( \mathcal{S} )
* 动作空间：( \mathcal{A} )
* 策略：( \pi(a|s) )
* 状态转移概率：( P(s'|s,a) )
* 即时奖励：( R(s,a,s') )（或其期望 ( R(s,a) )）
* 折扣因子：( \gamma \in [0,1) )

回报（Return）定义为：
[
G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
]

---

## 二、状态价值函数及其贝尔曼期望方程

### 1. 状态价值函数的定义

在策略 ( \pi ) 下，状态价值函数定义为：
[
V^\pi(s) \triangleq \mathbb{E}_\pi [G_t \mid S_t = s]
]

---

### 2. 推导思路

将回报拆分为“当前一步奖励 + 未来回报”：
[
G_t = R_{t+1} + \gamma G_{t+1}
]

代入定义：
[
\begin{aligned}
V^\pi(s)
&= \mathbb{E}*\pi [R*{t+1} + \gamma G_{t+1} \mid S_t = s] \
&= \mathbb{E}*\pi [R*{t+1} \mid S_t = s]

* \gamma \mathbb{E}*\pi [G*{t+1} \mid S_t = s]
  \end{aligned}
  ]

---

### 3. 展开条件期望

在状态 ( s ) 下，智能体按策略 ( \pi ) 选择动作 ( a )，环境转移到 ( s' )：

[
\begin{aligned}
V^\pi(s)
&= \sum_{a} \pi(a|s)
\sum_{s'} P(s'|s,a)
\left[ R(s,a,s') + \gamma V^\pi(s') \right]
\end{aligned}
]

---

### 4. 状态价值函数的贝尔曼期望方程

[
\boxed{
V^\pi(s)
========

\sum_{a} \pi(a|s)
\sum_{s'} P(s'|s,a)
\left[ R(s,a,s') + \gamma V^\pi(s') \right]
}
]

---

## 三、动作价值函数及其贝尔曼期望方程

### 1. 动作价值函数的定义

动作价值函数定义为：
[
Q^\pi(s,a) \triangleq \mathbb{E}_\pi [G_t \mid S_t = s, A_t = a]
]

---

### 2. 推导思路

同样拆分回报：
[
G_t = R_{t+1} + \gamma G_{t+1}
]

代入定义：
[
\begin{aligned}
Q^\pi(s,a)
&= \mathbb{E}*\pi [R*{t+1} + \gamma G_{t+1} \mid S_t = s, A_t = a] \
&= \mathbb{E}[R_{t+1} \mid s,a]

* \gamma \mathbb{E}*\pi [G*{t+1} \mid S_t = s, A_t = a]
  \end{aligned}
  ]

---

### 3. 对下一状态和下一动作取期望

执行 ( a ) 后转移到 ( s' )，随后按策略 ( \pi ) 选择 ( a' )：

[
\begin{aligned}
Q^\pi(s,a)
&=
\sum_{s'} P(s'|s,a)
\left[
R(s,a,s')
+
\gamma \sum_{a'} \pi(a'|s') Q^\pi(s',a')
\right]
\end{aligned}
]

---

### 4. 动作价值函数的贝尔曼期望方程

[
\boxed{
Q^\pi(s,a)
==========

\sum_{s'} P(s'|s,a)
\left[
R(s,a,s')
+
\gamma \sum_{a'} \pi(a'|s') Q^\pi(s',a')
\right]
}
]

---

## 四、补充说明（理解要点）

1. **“期望”来源**

   * 来自动作的不确定性（策略 ( \pi )）
   * 来自环境状态转移的不确定性（( P )）

2. **递归结构**

   * 当前价值 = 当前奖励 + 折扣后的下一时刻价值
   * 这是动态规划和强化学习算法（如策略评估、TD 学习）的理论基础

3. **与贝尔曼最优方程的区别**

   * 这里是“期望方程”，依赖给定策略 ( \pi )
   * 最优方程中使用的是 ( \max_a ) 而非策略期望

---

如果你希望，我可以进一步从**随机变量视角**、**矩阵形式**，或结合 **具体示例 MDP** 来重新推导一遍，以增强直观理解。
