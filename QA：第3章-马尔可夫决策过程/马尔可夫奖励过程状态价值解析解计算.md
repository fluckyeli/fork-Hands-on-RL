```text
解释下面代码所代表的公式
def compute(P, rewards, gamma, states_num):
    ''' 利用贝尔曼方程的矩阵形式计算解析解,states_num是MRP的状态数 '''
    rewards = np.array(rewards).reshape((-1, 1))  #将rewards写成列向量形式
    value = np.dot(np.linalg.inv(np.eye(states_num, states_num) - gamma * P),
                   rewards)
    return value
```

这段代码实现的是**马尔可夫奖励过程（Markov Reward Process, MRP）中状态价值函数的解析解计算**，使用的公式是**贝尔曼方程的矩阵形式**。

## 对应的数学公式

### 1. 贝尔曼方程的矩阵形式
对于马尔可夫奖励过程，状态价值函数的贝尔曼方程为：

\[
V = R + \gamma P V
\]

其中：
- \( V \) 是状态价值向量（每个状态一个值）
- \( R \) 是即时奖励向量
- \( P \) 是状态转移概率矩阵
- \( \gamma \) 是折扣因子

### 2. 解析解推导
将贝尔曼方程重写：

\[
V = R + \gamma P V
\]
\[
V - \gamma P V = R
\]
\[
(I - \gamma P) V = R
\]

最终得到解析解：

\[
V = (I - \gamma P)^{-1} R
\]

## 代码与公式的对应关系

```python
# 公式: V = (I - γP)⁻¹ R

# 1. 创建单位矩阵 I
np.eye(states_num, states_num)

# 2. 计算 I - γP
np.eye(states_num, states_num) - gamma * P

# 3. 求逆 (I - γP)⁻¹
np.linalg.inv(np.eye(states_num, states_num) - gamma * P)

# 4. 乘以奖励向量 R
np.dot(np.linalg.inv(np.eye(states_num, states_num) - gamma * P), rewards)

# 5. 返回结果 V
return value
```

## 参数说明
- `P`: 状态转移概率矩阵，\( P_{ij} \) 表示从状态 \( i \) 转移到状态 \( j \) 的概率
- `rewards`: 即时奖励向量，每个状态的即时奖励
- `gamma`: 折扣因子，\( 0 \leq \gamma \leq 1 \)
- `states_num`: 状态数量

## 注意事项
1. 这种方法只适用于**状态空间较小**的情况，因为矩阵求逆的计算复杂度为 \( O(n^3) \)
2. 需要确保 \( (I - \gamma P) \) 是可逆的（通常当 \( \gamma < 1 \) 时成立）
3. 这是**解析解**（精确解），而不是迭代近似解

这种方法直接计算出状态价值的精确值，而不需要通过迭代的方式逐步逼近。